{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_aR9xT5NrKC"
   },
   "source": [
    "# 3. Spark\n",
    "\n",
    "Spark Programming Guide: <https://spark.apache.org/docs/latest/> (use Python API recommended)\n",
    "Spark API: <https://spark.apache.org/docs/latest/api/python/index.html>\n",
    "\n",
    "\n",
    "# 3.1 Example Walkthrough\n",
    "3.1 Follow the Spark Examples below! After completion see Exercise 3.2 and 3.3!\n",
    "\n",
    "\n",
    "### Initialize PySpark\n",
    "\n",
    "First, we use the findspark package to initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40046,
     "status": "ok",
     "timestamp": 1615716675783,
     "user": {
      "displayName": "Andre Luckow",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64",
      "userId": "13897213013486729084"
     },
     "user_tz": -60
    },
    "id": "g5Zw7iFCN0ED",
    "outputId": "0555b7aa-9b20-4908-9fc3-a1423d8bc189"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pirTc4KfNrKG"
   },
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7473,
     "status": "ok",
     "timestamp": 1615716742055,
     "user": {
      "displayName": "Andre Luckow",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64",
      "userId": "13897213013486729084"
     },
     "user_tz": -60
    },
    "id": "PQrr41GwNrKI",
    "outputId": "a187f48e-d995-4db0-d397-7d5305823f4e"
   },
   "outputs": [],
   "source": [
    "# Initialize PySpark\n",
    "APP_NAME = \"PySpark Lecture\"\n",
    "SPARK_MASTER=\"local[1]\"\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import Row\n",
    "conf=pyspark.SparkConf()\n",
    "conf=pyspark.SparkConf().setAppName(APP_NAME).set(\"spark.local.dir\", os.path.join(os.getcwd(), \"tmp\"))\n",
    "sc = pyspark.SparkContext(master=SPARK_MASTER, conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "print(\"PySpark initiated...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGsih3q0NrKI"
   },
   "source": [
    "### Hello, World!\n",
    "\n",
    "Loading data, mapping it and collecting the records into RAM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1485,
     "status": "ok",
     "timestamp": 1615718551576,
     "user": {
      "displayName": "Andre Luckow",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64",
      "userId": "13897213013486729084"
     },
     "user_tz": -60
    },
    "id": "XSNJTxUOU7xG",
    "outputId": "2b279c5a-bac3-4043-fede-99ed74fcda3c"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/scalable-infrastructure/exercise-2023/main/data/example.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3280,
     "status": "ok",
     "timestamp": 1615716794158,
     "user": {
      "displayName": "Andre Luckow",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64",
      "userId": "13897213013486729084"
     },
     "user_tz": -60
    },
    "id": "Pt0wm3GQNrKI",
    "outputId": "83d24461-1f0b-44db-bd61-c0a3e116cd04"
   },
   "outputs": [],
   "source": [
    "# Load the text file using the SparkContext\n",
    "csv_lines = sc.textFile(\"example.csv\")\n",
    "\n",
    "# Map the data to split the lines into a list\n",
    "data = csv_lines.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Collect the dataset into local RAM\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6IFRE0kNrKJ"
   },
   "source": [
    "### Creating Objects from CSV\n",
    "\n",
    "Using a function with a map operation to create objects (dicts) as records..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1203,
     "status": "ok",
     "timestamp": 1615716800399,
     "user": {
      "displayName": "Andre Luckow",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64",
      "userId": "13897213013486729084"
     },
     "user_tz": -60
    },
    "id": "wFzFYn0MNrKJ",
    "outputId": "d3e023c8-6807-4013-a633-3f52ee418675"
   },
   "outputs": [],
   "source": [
    "# Turn the CSV lines into objects\n",
    "def csv_to_record(line):\n",
    "    parts = line.split(\",\")\n",
    "    record = {\n",
    "      \"name\": parts[0],\n",
    "      \"company\": parts[1],\n",
    "      \"title\": parts[2]\n",
    "    }\n",
    "    return record\n",
    "\n",
    "# Apply the function to every record\n",
    "records = csv_lines.map(csv_to_record)\n",
    "\n",
    "# Inspect the first item in the dataset\n",
    "records.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjMBodwrNrKJ"
   },
   "source": [
    "### GroupBy\n",
    "\n",
    "Using the groupBy operator to count the number of jobs per person..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1947,
     "status": "ok",
     "timestamp": 1615716805677,
     "user": {
      "displayName": "Andre Luckow",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64",
      "userId": "13897213013486729084"
     },
     "user_tz": -60
    },
    "id": "NfOqDcz6NrKJ",
    "outputId": "51336ca5-94a9-4b19-8a03-7a3aa8a30369"
   },
   "outputs": [],
   "source": [
    "# Group the records by the name of the person\n",
    "grouped_records = records.groupBy(lambda x: x[\"name\"])\n",
    "\n",
    "# Show the first group\n",
    "grouped_records.first()\n",
    "\n",
    "# Count the groups\n",
    "job_counts = grouped_records.map(\n",
    "  lambda x: {\n",
    "    \"name\": x[0],\n",
    "    \"job_count\": len(x[1])\n",
    "  }\n",
    ")\n",
    "\n",
    "job_counts.first()\n",
    "\n",
    "job_counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unQf6WJhNrKK"
   },
   "source": [
    "### Map vs FlatMap\n",
    "\n",
    "Understanding the difference between the map and flatmap operators..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0W5jyqtNrKK"
   },
   "outputs": [],
   "source": [
    "# Compute a relation of words by line\n",
    "words_by_line = csv_lines\\\n",
    "  .map(lambda line: line.split(\",\"))\n",
    "\n",
    "print(words_by_line.collect())\n",
    "\n",
    "# Compute a relation of words\n",
    "flattened_words = csv_lines\\\n",
    "  .map(lambda line: line.split(\",\"))\\\n",
    "  .flatMap(lambda x: x)\n",
    "\n",
    "flattened_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnQTDbiWNrKK"
   },
   "source": [
    "---\n",
    "## Further Exercises\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51092,
     "status": "ok",
     "timestamp": 1615718101014,
     "user": {
      "displayName": "Andre Luckow",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgX3nHdZvRUbdHrLYLXgFiGSepV9F6Hd9-YYk29L-Y=s64",
      "userId": "13897213013486729084"
     },
     "user_tz": -60
    },
    "id": "HrTk8P9iORnI",
    "outputId": "53e1cbc2-ad79-4bb9-c0e4-c7c8393b8e2c"
   },
   "outputs": [],
   "source": [
    "!wget \"https://github.com/scalable-infrastructure/exercise-2023/blob/main/data/nasa/NASA_access_log_Jul95.gz?raw=true\" -O NASA_access_log_Jul95.gz\n",
    "!gzip -d NASA_access_log_Jul95.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZO5zgmrNrKK"
   },
   "source": [
    "3.2 Implement a wordcount using Spark. How many words are in the file `example.csv`?\n",
    "\n",
    "3.3 Using the NASA Log file, implement a Spark version of the HTTP Response Code Analysis. How many log enteries per HTTP Response Code exist? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1uZRxt5TjGG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "fGsih3q0NrKI",
    "c6IFRE0kNrKJ"
   ],
   "name": "04_Spark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
