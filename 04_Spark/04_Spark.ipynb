{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Spark\n",
    "\n",
    "Spark Programming Guide: <https://spark.apache.org/docs/latest/> (use Python API recommended)\n",
    "Spark API: <https://spark.apache.org/docs/latest/api/python/index.html>\n",
    "\n",
    "\n",
    "# 3.1 Example Walkthrough\n",
    "3.1 Follow the Spark Examples below! After completion see Exercise 3.2 and 3.3!\n",
    "\n",
    "\n",
    "### Initialize PySpark\n",
    "\n",
    "First, we use the findspark package to initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark initiated...\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark\n",
    "#SPARK_MASTER=\"local[1]\"\n",
    "SPARK_MASTER=\"spark://mpp3r03c04s06.cos.lrz.de:7077\"\n",
    "APP_NAME = \"PySpark Lecture\"\n",
    "\n",
    "# If there is no SparkSession, create the environment\n",
    "try:\n",
    "    sc and spark\n",
    "except NameError as e:\n",
    "  #import findspark\n",
    "  #findspark.init()\n",
    "    import pyspark\n",
    "    import pyspark.sql\n",
    "    conf=pyspark.SparkConf().set(\"spark.cores.max\", \"4\")\n",
    "    sc = pyspark.SparkContext(master=SPARK_MASTER, conf=conf)\n",
    "    spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "print(\"PySpark initiated...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello, World!\n",
    "\n",
    "Loading data, mapping it and collecting the records into RAM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Russell Jurney', u'Relato', u'CEO'],\n",
       " [u'Florian Liebert', u'Mesosphere', u'CEO'],\n",
       " [u'Don Brown', u'Rocana', u'CIO'],\n",
       " [u'Steve Jobs', u'Apple', u'CEO'],\n",
       " [u'Donald Trump', u'The Trump Organization', u'CEO'],\n",
       " [u'Russell Jurney', u'Data Syndrome', u'Principal Consultant']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text file using the SparkContext\n",
    "csv_lines = sc.textFile(\"../data/example.csv\")\n",
    "\n",
    "# Map the data to split the lines into a list\n",
    "data = csv_lines.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Collect the dataset into local RAM\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Objects from CSV\n",
    "\n",
    "Using a function with a map operation to create objects (dicts) as records..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': u'Relato', 'name': u'Russell Jurney', 'title': u'CEO'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the CSV lines into objects\n",
    "def csv_to_record(line):\n",
    "    parts = line.split(\",\")\n",
    "    record = {\n",
    "      \"name\": parts[0],\n",
    "      \"company\": parts[1],\n",
    "      \"title\": parts[2]\n",
    "    }\n",
    "    return record\n",
    "\n",
    "# Apply the function to every record\n",
    "records = csv_lines.map(csv_to_record)\n",
    "\n",
    "# Inspect the first item in the dataset\n",
    "records.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy\n",
    "\n",
    "Using the groupBy operator to count the number of jobs per person..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'job_count': 1, 'name': u'Donald Trump'},\n",
       " {'job_count': 1, 'name': u'Don Brown'},\n",
       " {'job_count': 1, 'name': u'Florian Liebert'},\n",
       " {'job_count': 1, 'name': u'Steve Jobs'},\n",
       " {'job_count': 2, 'name': u'Russell Jurney'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the records by the name of the person\n",
    "grouped_records = records.groupBy(lambda x: x[\"name\"])\n",
    "\n",
    "# Show the first group\n",
    "grouped_records.first()\n",
    "\n",
    "# Count the groups\n",
    "job_counts = grouped_records.map(\n",
    "  lambda x: {\n",
    "    \"name\": x[0],\n",
    "    \"job_count\": len(x[1])\n",
    "  }\n",
    ")\n",
    "\n",
    "job_counts.first()\n",
    "\n",
    "job_counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map vs FlatMap\n",
    "\n",
    "Understanding the difference between the map and flatmap operators..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'Russell Jurney', u'Relato', u'CEO'], [u'Florian Liebert', u'Mesosphere', u'CEO'], [u'Don Brown', u'Rocana', u'CIO'], [u'Steve Jobs', u'Apple', u'CEO'], [u'Donald Trump', u'The Trump Organization', u'CEO'], [u'Russell Jurney', u'Data Syndrome', u'Principal Consultant']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'Russell Jurney',\n",
       " u'Relato',\n",
       " u'CEO',\n",
       " u'Florian Liebert',\n",
       " u'Mesosphere',\n",
       " u'CEO',\n",
       " u'Don Brown',\n",
       " u'Rocana',\n",
       " u'CIO',\n",
       " u'Steve Jobs',\n",
       " u'Apple',\n",
       " u'CEO',\n",
       " u'Donald Trump',\n",
       " u'The Trump Organization',\n",
       " u'CEO',\n",
       " u'Russell Jurney',\n",
       " u'Data Syndrome',\n",
       " u'Principal Consultant']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute a relation of words by line\n",
    "words_by_line = csv_lines\\\n",
    "  .map(lambda line: line.split(\",\"))\n",
    "\n",
    "print(words_by_line.collect())\n",
    "\n",
    "# Compute a relation of words\n",
    "flattened_words = csv_lines\\\n",
    "  .map(lambda line: line.split(\",\"))\\\n",
    "  .flatMap(lambda x: x)\n",
    "\n",
    "flattened_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Further Exercises\n",
    "\n",
    "3.2 Implement a wordcount using Spark. Who many words are in the file `example.csv`?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 How many log enteries per HTTP Response Code exist? "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
