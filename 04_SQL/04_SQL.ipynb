{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"dkASU227TuUJ"},"source":["# 4.SQL and Dataframes\n","\n","References:\n","\n","* Spark-SQL, <https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes>\n","\n","\n","# 4.1  Example Walkthrough\n","Follow the Spark SQL and Dataframes Examples below!\n","\n","### Initialize PySpark\n","\n","First, we use the findspark package to initialize PySpark."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38298,"status":"ok","timestamp":1617112910170,"user":{"displayName":"","photoUrl":"","userId":""},"user_tz":-120},"id":"LbZC0TEZTuUL","outputId":"bcef61e9-dc5a-44a3-eff4-2943686c6b05"},"outputs":[],"source":["!pip install -q pyspark"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":673,"status":"ok","timestamp":1617113437577,"user":{"displayName":"","photoUrl":"","userId":""},"user_tz":-120},"id":"PiPbzzOQcORs","outputId":"190e9f8d-15e4-4ca6-9241-465753021da3"},"outputs":[],"source":["!cat /proc/cpuinfo"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6579,"status":"ok","timestamp":1617113444953,"user":{"displayName":"","photoUrl":"","userId":""},"user_tz":-120},"id":"1hiLvAIOTuUL","outputId":"b291a61b-2653-4758-dcc3-545ff3f96065"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]},{"name":"stdout","output_type":"stream","text":["23/03/04 18:47:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","23/03/04 18:47:30 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n","PySpark initiated...\n"]}],"source":["# Initialize PySpark\n","import os, sys\n","APP_NAME = \"PySpark Lecture\"\n","SPARK_MASTER=\"local[2]\"\n","import pyspark\n","import pyspark.sql\n","from pyspark.sql import Row\n","conf=pyspark.SparkConf()\n","conf=pyspark.SparkConf().setAppName(APP_NAME).set(\"spark.local.dir\", os.path.join(os.getcwd(), \"tmp\"))\n","sc = pyspark.SparkContext(master=SPARK_MASTER, conf=conf)\n","spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n","\n","print(\"PySpark initiated...\")"]},{"cell_type":"markdown","metadata":{"id":"29hDbwJgTuUM"},"source":["### Hello, World!\n","\n","Loading data, mapping it and collecting the records into RAM..."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3446,"status":"ok","timestamp":1617113444953,"user":{"displayName":"","photoUrl":"","userId":""},"user_tz":-120},"id":"sdkuaf_VVUxy","outputId":"5a714bb1-435d-435b-8a42-86ed761d2650"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-03-04 18:47:50--  https://raw.githubusercontent.com/scalable-infrastructure/exercise-2023/main/data/example.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 189 [text/plain]\n","Saving to: 'example.csv'\n","\n","example.csv         100%[===================>]     189  --.-KB/s    in 0s      \n","\n","2023-03-04 18:47:50 (5.01 MB/s) - 'example.csv' saved [189/189]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/scalable-infrastructure/exercise-2023/main/data/example.csv"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2270,"status":"ok","timestamp":1617113464836,"user":{"displayName":"","photoUrl":"","userId":""},"user_tz":-120},"id":"DaNcpjX0TuUM","outputId":"7ec3dfde-2712-4dd6-bd2d-d278ceeabe18"},"outputs":[{"data":{"text/plain":["[['Russell Jurney', 'Relato', 'CEO'],\n"," ['Florian Liebert', 'Mesosphere', 'CEO'],\n"," ['Don Brown', 'Rocana', 'CIO'],\n"," ['Steve Jobs', 'Apple', 'CEO'],\n"," ['Donald Trump', 'The Trump Organization', 'CEO'],\n"," ['Russell Jurney', 'Data Syndrome', 'Principal Consultant']]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Load the text file using the SparkContext\n","csv_lines = sc.textFile(\"example.csv\")\n","\n","# Map the data to split the lines into a list\n","data = csv_lines.map(lambda line: line.split(\",\"))\n","\n","# Collect the dataset into local RAM\n","data.collect()"]},{"cell_type":"markdown","metadata":{"id":"rKlgdJVdTuUM"},"source":["### Creating Rows\n","\n","Creating `pyspark.sql.Rows` out of your data so you can create DataFrames..."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":649,"status":"ok","timestamp":1617113556308,"user":{"displayName":"","photoUrl":"","userId":""},"user_tz":-120},"id":"S3ZDj3uvTuUN"},"outputs":[],"source":["# Convert the CSV into a pyspark.sql.Row\n","def csv_to_row(line):\n","    parts = line.split(\",\")\n","    row = Row(\n","      name=parts[0],\n","      company=parts[1],\n","      title=parts[2]\n","    )\n","    return row\n","\n","# Apply the function to get rows in an RDD\n","rows = csv_lines.map(csv_to_row)"]},{"cell_type":"markdown","metadata":{"id":"sRmw3MIBTuUN"},"source":["### Creating DataFrames from RDDs\n","\n","Using the `RDD.toDF()` method to create a dataframe, registering the `DataFrame` as a temporary table with Spark SQL, and counting the jobs per person using Spark SQL."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10320,"status":"ok","timestamp":1617113568045,"user":{"displayName":"","photoUrl":"","userId":""},"user_tz":-120},"id":"R0Xj9BfITuUN","outputId":"1b6ec660-a930-45ea-9d2c-2f865ad707fc"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/q224516/anaconda3/envs/aaml2023/lib/python3.10/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n","  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"]},{"name":"stdout","output_type":"stream","text":["+---------------+-----+\n","|           name|total|\n","+---------------+-----+\n","|Florian Liebert|    1|\n","|      Don Brown|    1|\n","| Russell Jurney|    2|\n","|     Steve Jobs|    1|\n","|   Donald Trump|    1|\n","+---------------+-----+\n","\n"]},{"data":{"text/plain":["[Row(name='Florian Liebert', total=1),\n"," Row(name='Don Brown', total=1),\n"," Row(name='Russell Jurney', total=2),\n"," Row(name='Steve Jobs', total=1),\n"," Row(name='Donald Trump', total=1)]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Convert to a pyspark.sql.DataFrame\n","rows_df = rows.toDF()\n","\n","# Register the DataFrame for Spark SQL\n","rows_df.registerTempTable(\"executives\")\n","\n","# Generate a new DataFrame with SQL using the SparkSession\n","job_counts = spark.sql(\"\"\"\n","SELECT\n","  name,\n","  COUNT(*) AS total\n","  FROM executives\n","  GROUP BY name\n","\"\"\")\n","job_counts.show()\n","\n","# Go back to an RDD\n","job_counts.rdd.collect()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2ofD74jfTuUO"},"source":["# 4.2-4.9 NASA DataSet\n","\n","4.2 Create a Spark-SQL table with fields for IP/Host and Response Code from the NASA Log file! "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uELQAPjYTuUO"},"source":["4.3 Run an SQL query that outputs the number of occurrences of each HTTP response code!\n","\n","4.4 Implement the same Query using the Dataframe API!"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"c22wAS25TuUO"},"source":["4.5 Cachen Sie den Dataframe und f端hren Sie dieselbe Query nochmals aus! Messen Sie die Laufzeit f端r das Cachen und f端r die Ausf端hrungszeit der Query!"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DfsKn2IGTuUP"},"source":["4.6 Performance Analysis - Weak Scaling: \n","* Create RDDs with 2x, 4x, 8x and 16x of the size of the NASA log dataset! Persist the dataset in the Spark Cache! Use an appropriate number of cores (e.g. 8 or 16)!\n","* Measure and plot the response times for all datasets using a constant number of cores!\n","* Plot the results!\n","* Explain the results!\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"U2j_E8RSTuUP"},"source":["4.7 Performance Analysis - Weak Scaling: \n","\n","  * **Measure the runtime for the query for 1, 2, 4 worker threads (local[n]) for 1x and 16x datasets!** Datasets cached in Memory! Note that Collab environment only has two cores!\n","  * Compute the speedup and efficiency!\n","  * Plot the responses!\n","  * Explain the results!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_74seWVb9kW"},"outputs":[],"source":[]}],"metadata":{"anaconda-cloud":{},"colab":{"collapsed_sections":[],"name":"Copy of 05_SQL.ipynb","provenance":[{"file_id":"https://github.com/scalable-infrastructure/exercise-students-2021/blob/master/05_SQL/05_SQL.ipynb","timestamp":1617113592153}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
